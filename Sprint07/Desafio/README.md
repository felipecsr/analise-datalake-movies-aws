
# üéØ Objetivo üî¥

Este README, da sprint 07, √© o primeiro de cinco etapas que comp√µem o **Desafio Final Programa de Bolsas da Compass UOL**.  

**O desafio, em resumo**, (que come√ßa na sprint 06 e termina na sprint 10) consiste na cria√ß√£o de dashboards para an√°lise de dados provenientes de um Data Lake - que estamos come√ßando a construir nesta sprint. Todo o processo envolver√° a troca de arquivos locais, execu√ß√£o de scripts em ambiente local e na nuvem, extra√ß√£o de dados de APIs p√∫blicas, tratamentos diversos, com o objetivo final de integrar ao Console AWS, e l√° criar visualiza√ß√µes anal√≠ticas.  

**Como primeira etapa do desafio**, nesta sprint, recebemos dois arquivos `csv` contendo dados sobre filmes e s√©ries, com informa√ß√µes diversas desse universo. Inicialmente, fomos orientados a explorar esses dados, levando em considera√ß√£o as possibilidades adicionais que surgir√£o ao extrair informa√ß√µes via API do [site TMDB](https://www.themoviedb.org/?language=pt-br).   

Ap√≥s a an√°lise do conjunto de dados e das [possibilidades de dados extra√≠veis do TMDB](../Desafio/Lista%20de%20possibilidades%20TMBD%20-%20Preparado%20por%20Felipe%20Reis%20-%20Planilhas%20Google.pdf), o ponto de partida dos trabalhos seria a elabora√ß√£o de perguntas a serem respondidas ao final do desafio, que orientar√£o todo o processo. A seguir, apresentamos algumas perguntas direcionadoras.

>Para a minha squad, n√∫mero 2, o tema s√£o s√©ries e filmes do g√™nero **Crime e Guerra**.

<br/>

## ‚ùì Perguntas direcionadoras do desafio

1) Dentro do segmento `Crime/ Guerra`, quais os 10 ou 20 filmes mais votados(`numeroVotos`) e melhores votados (`notaMedia`)?

2) Destes, existe algum ator ou atriz (`nomeArtista`) que aparece em mais de um filme?

    *Obs: eventualmente inverter 2 e 1, para caso a amostragem seja invi√°vel nessa disposi√ß√£o apresentada.* 

3) Destes, quais os atores/atrizes que est√£o vivos (`anoFalecimento`), de acordo com a base?

4) Destes, com os dados agregados do TMDB, quais s√£o os filmes com melhor resultado (`revenue` - `budget`), ou seja que a receita cobre o or√ßamento?

5) Outra an√°lise que √© poss√≠vel √© de valor m√©dio de or√ßamento x receita (ou diferen√ßa = resultado), na linha do tempo, talvez anualmente ou por d√©cadas.

<br/>

## ‚ñ∂Ô∏è Resolu√ß√£o do desafio!

### üêç Cria√ß√£o do script em Python

A primeira etapa foi a cria√ß√£o do script com o seguinte escopo:

- **Configura vari√°veis** com o nome do bucket S3 e os caminhos dos arquivos CSV.
- **L√™ e valida** os arquivos CSV com `pandas`.
- **Verifica e cria** o bucket S3 caso n√£o exista.
- **Faz upload** dos arquivos CSV para o S3.

*Obs: realizei o teste localmente antes da cria√ß√£o e execu√ß√£o do container, para primeiro teste.*

```python
import boto3
from botocore.exceptions import ProfileNotFound
import pandas as pd
from pathlib import Path
from datetime import datetime

# Configura√ß√µes principais
BUCKET_NAME = "desafio-filmes-series"

# Gerar automaticamente os caminhos de pasta com base na data atual
today = datetime.now()
year, month, day = today.strftime("%Y"), today.strftime("%m"), today.strftime("%d")
RAW_PATH_MOVIES = f"Raw/Local/CSV/Movies/{year}/{month}/{day}/movies.csv"
RAW_PATH_SERIES = f"Raw/Local/CSV/Series/{year}/{month}/{day}/series.csv"

# Fun√ß√£o para carregar credenciais explicitamente
def load_credentials():
    try:
        session = boto3.Session(profile_name=None)  # Usar o perfil padr√£o
        print("Credenciais carregadas do perfil padr√£o.")
    except ProfileNotFound:
        raise Exception("N√£o foi poss√≠vel carregar credenciais do perfil padr√£o.")

# Fun√ß√£o para upload de arquivos ao S3
def upload_to_s3(file_path, bucket, key):
    print(f"Tentando fazer upload do arquivo {file_path} para s3://{bucket}/{key}")
    s3_client = boto3.client('s3')
    try:
        s3_client.upload_file(file_path, bucket, key)
        print(f"Arquivo {file_path} enviado com sucesso para s3://{bucket}/{key}")
    except Exception as e:
        raise Exception(f"Erro ao enviar {file_path} para o S3: {e}")

# Fun√ß√£o para verificar/criar bucket
def ensure_bucket_exists(bucket_name):
    print(f"Verificando se o bucket '{bucket_name}' existe...")
    s3_client = boto3.client('s3')
    try:
        # Verificar se o bucket existe
        s3_client.head_bucket(Bucket=bucket_name)
        print(f"Bucket '{bucket_name}' j√° existe.")
    except boto3.exceptions.botocore.exceptions.ClientError as e:
        error_code = e.response['Error']['Code']
        if error_code == '404':
            print(f"Bucket '{bucket_name}' n√£o encontrado. Criando...")
            try:
                # Verificar a regi√£o e ajustar o LocationConstraint
                region = s3_client.meta.region_name
                if region == "us-east-1":
                    s3_client.create_bucket(Bucket=bucket_name)
                else:
                    s3_client.create_bucket(
                        Bucket=bucket_name,
                        CreateBucketConfiguration={
                            'LocationConstraint': region
                        }
                    )
                print(f"Bucket '{bucket_name}' criado com sucesso.")
            except Exception as create_error:
                raise Exception(f"Erro ao criar o bucket: {create_error}")
        elif error_code == '403':
            print(f"Permiss√£o negada para verificar o bucket '{bucket_name}'.")
            raise
        else:
            raise Exception(f"Erro ao verificar o bucket: {e}")

# Fun√ß√£o para validar os arquivos locais
def validate_files(*file_paths):
    print("Validando a exist√™ncia dos arquivos...")
    for file_path in file_paths:
        if not Path(file_path).is_file():
            print(f"Erro: Arquivo {file_path} n√£o encontrado.")
            return False
    print("Todos os arquivos foram encontrados.")
    return True

# Fun√ß√£o para leitura e valida√ß√£o de dados
def read_and_validate_csv(file_path):
    print(f"Tentando ler o arquivo {file_path} com pandas...")
    try:
        # Ler o arquivo CSV especificando o separador | e desativar "low_memory"
        df = pd.read_csv(file_path, sep="|", low_memory=False)
        print(f"Arquivo {file_path} carregado com sucesso. Total de registros: {len(df)}")
        return df
    except Exception as e:
        print(f"Erro ao ler o arquivo {file_path}: {e}")
        return None

# Fun√ß√£o principal
def main():
    print("Iniciando o script...")
    
    # Carregar credenciais explicitamente
    load_credentials()

    # Caminhos locais dos arquivos CSV
    local_movies_path = "data/movies.csv"
    local_series_path = "data/series.csv"

    # Validar se os arquivos existem
    if not validate_files(local_movies_path, local_series_path):
        print("Erro: Alguns arquivos n√£o foram encontrados. Encerrando.")
        return

    # Ler os arquivos CSV e validar o conte√∫do
    print("Lendo e validando os arquivos CSV...")
    movies_df = read_and_validate_csv(local_movies_path)
    series_df = read_and_validate_csv(local_series_path)

    # Caso algum arquivo n√£o tenha sido lido, interrompe o processo
    if movies_df is None or series_df is None:
        print("Erro: N√£o foi poss√≠vel ler os arquivos CSV. Upload interrompido.")
        return

    # Verificar e criar o bucket, se necess√°rio
    ensure_bucket_exists(BUCKET_NAME)

    # Upload para o S3
    print("Iniciando upload para o S3...")
    success = True
    try:
        upload_to_s3(local_movies_path, BUCKET_NAME, RAW_PATH_MOVIES)
    except Exception as e:
        success = False
        print(f"Falha no upload de {local_movies_path}: {e}")
    
    try:
        upload_to_s3(local_series_path, BUCKET_NAME, RAW_PATH_SERIES)
    except Exception as e:
        success = False
        print(f"Falha no upload de {local_series_path}: {e}")

    if success:
        print("Upload conclu√≠do com sucesso!")
    else:
        print("Erro: Um ou mais uploads falharam. Verifique os logs acima.")

# Executar o script
if __name__ == "__main__":
    main()
```
<br/>

### üêã Cria√ß√£o do Dockerfile
As instru√ß√µes do desafio solicitam a cria√ß√£o de um container para execu√ß√£o do script acima, com inten√ß√£o de isolar os efeitos do ambiente local, garantindo possibilidade de replicar (via container) em qualquer m√°quina!

```dockerfile
# Usar imagem base completa do Python
FROM python:3.9

# Instalar depend√™ncias Python
RUN pip install --no-cache-dir pandas boto3

# Definir diret√≥rio de trabalho no container
WORKDIR /app

# Copiar os arquivos do projeto para o container
COPY . /app

# Comando padr√£o para executar o script Python
CMD ["python", "script.py"]
```

*Obs: pensei em utilizar a imagem do Python 3.9 Slim, para otimizar ainda o espa√ßo de armazenamento, mas conversando com o monitor Gilson, entendemos que os megabytes que economizaria, n√£o seriam suficientemente relevantes - al√©m de complexizar um pouco a instala√ß√£o de bibliotecas. Segui com a vers√£o normal, o Python 3.9.*

<br/>

### üêö Encapsulamento com Shell

Ao final improvisei criando um script Shell para encapsular uma execu√ß√£o de terminal que foi necess√°ria, pra mim, por conta das credenciais de login da AWS, via SSO. 

Por algum motivo que n√£o consegui ainda compreender, minhas chaves quando atualizadas ficam num "cache virtual" e n√£o no arquivo de texto `credentials` e `config`. Para contornar, precisaria sempre executar o Docker (run) com mais alguns par√¢metros. Para facilitar a execu√ß√£o, encapsulei!

```shell
#!/bin/bash

docker run \
  -e AWS_ACCESS_KEY_ID="$AWS_ACCESS_KEY_ID" \
  -e AWS_SECRET_ACCESS_KEY="$AWS_SECRET_ACCESS_KEY" \
  -e AWS_SESSION_TOKEN="$AWS_SESSION_TOKEN" \
  -v "$(pwd)/data:/app/data" \
  data-lake-uploader
```
<br/>

### ü§ñ Cria√ß√£o do container e execu√ß√£o

Ap√≥s os c√≥digos criados, bastou a cria√ß√£o do container em si, e a sua execu√ß√£o atrav√©s do Shell. O container ao ser executado, automaticamente executa o script Python. Ao verificarmos o bucket criado, os arquivos armazenados na estrutura de pastas sugerida nas instru√ß√µes, conclu√≠ que atingi o ponto final neste desafio!

>Estrutura de pastas:
![estrutura para filmes](../evidencias/desafio/estrutura-movies.png)
![estrutura para series](../evidencias/desafio/estrutura-series.png)

<br/>

## üìå Considera√ß√µes finais sobre a Sprint 06

A Sprint 06 foi marcada pela integra√ß√£o entre v√°rias tecnologias e pela constru√ß√£o de um pipeline completo de dados, desde a manipula√ß√£o inicial de arquivos locais at√© o upload para o S3, com o uso de cont√™ineres Docker para garantir a reprodutibilidade e consist√™ncia do ambiente de execu√ß√£o. Essa etapa foi fundamental para consolidar o aprendizado em Python, pandas e AWS, especialmente ao lidar com grandes volumes de dados e interagir diretamente com os servi√ßos da nuvem, utilizando o `boto3`.

Al√©m disso, foi uma oportunidade de aplicar conhecimentos adquiridos anteriormente, como a utiliza√ß√£o de credenciais no AWS SSO e a automa√ß√£o do processo de execu√ß√£o do script em diferentes m√°quinas via Docker. A cria√ß√£o de uma estrutura eficiente de pastas e o uso de cont√™ineres demonstraram a import√¢ncia de um ambiente controlado e facilmente replic√°vel, fundamental para garantir o sucesso no gerenciamento de dados em nuvem.

Estou satisfeito com o progresso realizado at√© aqui, tanto no desenvolvimento t√©cnico quanto na compreens√£o do fluxo de trabalho. Esta sprint me preparou para os desafios seguintes e para aprofundar meu entendimento sobre pipelines de dados e integra√ß√£o com a AWS. Estou motivado para seguir adiante e continuar expandindo meus conhecimentos enquanto trabalho nas pr√≥ximas etapas do projeto. üöÄ