# üíª Exerc√≠cios

Nesta sprint 07, seguimos com a segunda etapa de constru√ß√£o do **Desafio Final** do Programa de Bolsas da Compass UOL - a etapa 2 de 5.

Nesta sprint o tema principal √© o `Apache Spark`e suas utiliza√ß√µes para manipula√ß√£o e an√°lise de dados. Entre suas principais caracter√≠sticas est√° o processamento distribu√≠do e paralelo, que √© essencial para volumes na ordem dos gigabytes e terabytes, os Data Lakes. H√° outras caracter√≠sticas importantes tamb√©m como:

- escalabilidade horizontal;
- integra√ß√£o com Python e SQL;
- aceita dados semi e n√£o estruturados - al√©m, claro, dos estruturados;
- trabalha (entre outras) com extens√µes como `ORC` e `Parquet` que s√£o otimizadas para velocidade de processamento e compacta√ß√£o no armazenamento;
- integr√°veis com servi√ßos em nuvem, com a AWS.

<br/>

Abaixo veremos os exerc√≠cios realizados nesta sprint.

## 1 - Exerc√≠cio: Contador de Palavras com Apache Spark ‚ú® e Jupyter Lab ü™ê

O objetivo deste exerc√≠cio foi utlizarmos o `Pyspark`para uma an√°lise simples, de contagem de palavras num determinado arquivo - o README da nossa sprint. Em outras palavras, ao inv√©s da original do `Apache Spark`, a linguagem `Scala`, utilizaremos o Pyspark, que de forma nativa tamb√©m, t√™m Python e SQL como possibilidades.

### 1.1 - Prepara√ß√£o do Docker

1. Pull da imagem Docker + Jupyter
![Docker Image](../Sprint07/evidencias/ex5-spark-jupyter/1-dockerpull.png)

<br/>

2. Execu√ß√£o do Jupyter, via docker, com os par√¢metros de porta e path ajustados 

``` Shell
docker run -it --rm \
    -p 8888:8888 \
    -v /home/fcsr/Documentos/Nabucodonossor-workspace/PB-FELIPE-REIS/Sprint07/exercicios/5-Apache_Spark_Contador_de_Palavras:/home/jovyan/work \
    jupyter/all-spark-notebook
```

![Jupyter via Docker](../Sprint07/evidencias/ex5-spark-jupyter/2-jupyter_via_docker.png)

![Jupyter no navegador](../Sprint07/evidencias/ex5-spark-jupyter/3-jupyter_interface.png)

<br/>

3. Testes de execu√ß√£o da Spark Session e reflexo entre diret√≥rio do docker e meu ambiente local: sucesso!

![teste jupyter](../Sprint07/evidencias/ex5-spark-jupyter/4-teste_jupyter.png)

![teste jupyter](../Sprint07/evidencias/ex5-spark-jupyter/5-reflexo_docker_local.png)

<br/>

### 1.2 - Execu√ß√£o dos comandos via Pyspark + Resultado

1. Execu√ß√£o do docker no modo interativo
```bash
docker run -it --rm \
    -v /home/fcsr/Documentos/Nabucodonossor-workspace/PB-FELIPE-REIS/Sprint07/exercicios/5-Apache_Spark_Contador_de_Palavras:/home/jovyan/work \
    jupyter/all-spark-notebook /bin/bash
```

<br/>

2. Download do README.md para o diret√≥rio do docker
```bash
wget --header="Authorization: token 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" \
https://raw.githubusercontent.com/felipecsr/PB-FELIPE-REIS/refs/heads/main/README.md -O /home/jovyan/work/README.md
```
>  **Obs:** *foi necess√°rio criar um token via interface do Github, que utilizei na execu√ß√£o do wget no terminal, e apesar de descrito aqui no c√≥digo/ documental√£o/ print, j√° foi deletado/ expirado e por isso mantive.*

![wget sucesso](../Sprint07/evidencias/ex5-spark-jupyter/6-wget.png)

<br/>

3. C√≥digo contador de palavras executado no Pyspark

``` python
import os
import glob
import shutil
from pyspark.sql import SparkSession

# 1. Inicializar a SparkSession
spark = SparkSession.builder \
    .appName("Word Count Exercise") \
    .master("local[*]") \
    .getOrCreate()

# 2. Definir o caminho absoluto do arquivo README.md
file_path = "/home/jovyan/work/README.md"

# 3. Carregar o arquivo README.md como RDD
rdd = spark.sparkContext.textFile(file_path)

# 4. Contar as palavras no arquivo (preservando a ordem de primeira apari√ß√£o)
word_counts_with_order = (rdd.flatMap(lambda line: line.split())    # Quebra linhas em palavras
                              .zipWithIndex()                      # Associa cada palavra a seu √≠ndice global
                              .map(lambda word_idx: (word_idx[0], (1, word_idx[1])))  # Formato (palavra, (1, √≠ndice))
                              .reduceByKey(lambda acc, val: (acc[0] + val[0], min(acc[1], val[1])))  # Soma contagens, mant√©m o menor √≠ndice
                              .sortBy(lambda word_idx: word_idx[1][1])  # Ordena pelo √≠ndice de apari√ß√£o
                              .map(lambda word_idx: (word_idx[0], word_idx[1][0])))  # Resultado final: (palavra, contagem)

# 5. Converter para DataFrame
word_counts_df = word_counts_with_order.toDF(["word", "count"])

# 6. Salvar como CSV em uma √∫nica parti√ß√£o
temp_output_path = "/home/jovyan/work/results/temp_word_counts"
word_counts_df.coalesce(1).write.csv(temp_output_path, header=True, mode="overwrite")

# 7. Renomear o arquivo CSV gerado para um nome mais intuitivo
csv_part_file = glob.glob(temp_output_path + "/part-*.csv")[0]  # Busca o arquivo CSV na pasta
final_csv_file = "/home/jovyan/work/results/word_counts_final.csv"

shutil.move(csv_part_file, final_csv_file)  # Renomeia o arquivo
shutil.rmtree(temp_output_path)  # Remove a pasta tempor√°ria

print(f"Contagem de palavras conclu√≠da e salva como um √∫nico arquivo CSV em {final_csv_file}")
```
<br/>

Al√©m do c√≥digo acima, registrei a execu√ß√£o do script via Docker > Pyspark:
![sucesso script](../Sprint07/evidencias/ex5-spark-jupyter/7-pyspark-sucesso.png)


4. Resultado obtido

E por fim, neste exerc√≠cio, o resultado obtido de acordo com o que desenvolvi no script foi um arquivo `csv` que pode ser [consultado aqui neste link](../Sprint07/exercicios/5-Apache_Spark/results/word_counts_final.csv).

> **Obs:** *foi interessante verificar, durante as diversas tentativas de resolu√ß√£o do exerc√≠cio, o retorno do Spark com arquivos 'particionados', por exemplo 2 arquivos.crc (com os metadados) e outros 2 arquivos.csv - que √© demonstra√ß√£o cabal de sua form distribu√≠da de processamentos!*

<br/>

## 2 - Exerc√≠cio: TMDB üçøüìΩÔ∏è
Neste exerc√≠cio, o objetivo foi realizar uma consulta ao agregador de informa√ß√µes de filmes e s√©ries, [TMDB (The Movie Data Base)](https://www.themoviedb.org/?language=pt-BR), via sua API p√∫blica.

1. Foi criada uma conta gratuita com meus dados pessoais;
2. Depois solicitei a libera√ß√£o de uma chave e token, atrav√©s da √°rea voltada para desenvolvedores - com √™xito!
3. Constru√≠ um c√≥digo semelhante ao do exemplo do exerc√≠cio, apenas para teste simples.
```python
import requests
import pandas as pd
from IPython.display import display
from dotenv import load_dotenv
import os

# Carregar vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obter a chave de API do TMDB
api_key = os.getenv("TMDB_API_KEY")

# Verificar se a chave foi carregada corretamente
if not api_key:
    raise ValueError("Chave de API n√£o encontrada. Verifique o arquivo .env.")

# URL da API (ajustado para Crime e Guerra)
url = f"https://api.themoviedb.org/3/movie/top_rated?api_key={api_key}&language=pt-BR"

# Fazer a requisi√ß√£o
response = requests.get(url)
data = response.json()

# Lista para armazenar os filmes
filmes = []

# Coletar os primeiros 30 registros diretamente
for movie in data['results'][:30]:
    df = {
        'T√≠tulo': movie['title'],
        'Data de Lan√ßamento': movie['release_date'],
        'Vis√£o Geral': movie['overview'],
        'Votos': movie['vote_count'],
        'M√©dia de Votos': movie['vote_average']
    }
    filmes.append(df)

# Criar DataFrame
df = pd.DataFrame(filmes)

# Exibir DataFrame
display(df)
```
> **Obs:** pesquisando utilizei uma biblioteca e m√©todo que tornam as chaves ocultas no c√≥digo, para dar mais seguran√ßa e praticidade. Trata-se do biblioteca `dotenv`. O funcionamento √© simples: cria-se um arquivo .env que ter√° a chave/ senha  /  o c√≥digo consulta este arquivo e consulta a senha apenas na mem√≥ria (de forma oculta ao usu√°rio) e a liga√ß√£o com a API fica funcional. E para que n√£o suba para o repositorio no `commit` utilizei o `.gitignore`.  

<br/>

4. Tivemos √™xito na consulta, que ficou dispon√≠vel no pr√≥prio terminal atrav√©s da fun√ß√£o `display`.
![consulta API TMDB](../Sprint07/evidencias/ex6-TMDB/display-sucessp.png)

<br/><br/>

## 3 - Exerc√≠cio: AWS Glue üîªüîéüìä

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>
<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>



# üìú Certificados

- [Fundamentals of Analytics - Part 1](../Sprint06/certificados/Analyticsp1.png)
- [Fundamentals of Analytics - Part 2](../Sprint06/certificados/Analyticsp2.png)
- [Introduction to Amazon Athena](../Sprint06/certificados/Athena.png)
- [Amazon EMR](../Sprint06/certificados/EMR.png)
- [AWS Glue Getting Started](../Sprint06/certificados/Glue.png)
- [Getting Started with Amazon Redshift](../Sprint06/certificados/Redshift.png)
- [Best Practices for Data Warehousing with Amazon Redshift](../Sprint06/certificados/Redshift-DW.png)
- [Serverless Analytics](../Sprint06/certificados/Analyticsp1.png)
- [Amazon QuickSight - Getting Started](../Sprint06/certificados/QuickSight.png)

<br/>  
  
# üß† Desafio
**AWS S3, Containers e Python 3.9: Automatizando o Pipeline de Dados**  
O desafio dessa sprint foi uma experi√™ncia interessante e desafiadora no uso de AWS S3 e containers, com um foco em automa√ß√£o e estrutura√ß√£o de dados na nuvem. O objetivo foi organizar e enviar dois arquivos CSV, movies.csv e series.csv, para um bucket no S3, seguindo uma estrutura espec√≠fica que foi detalhada nos slides do desafio. Esses arquivos foram alocados em uma zona de raw data, criando um ponto inicial para an√°lises futuras.

O que realmente me impressionou nesta sprint foi a orienta√ß√£o para utilizar um container com Python 3.9. Essa abordagem, ao inv√©s de usar o ambiente local, foi uma maneira muito eficiente de garantir que o c√≥digo tivesse as depend√™ncias necess√°rias e funcionasse de maneira consistente em diferentes ambientes. Al√©m disso, ao utilizar um container, a solu√ß√£o ficou mais "flat", sem depender das configura√ß√µes espec√≠ficas da minha m√°quina, o que facilita a replica√ß√£o em diferentes contextos, seja para testes ou produ√ß√£o.

Por fim, um script em Python foi criado para automatizar todo o processo: desde a cria√ß√£o do bucket no S3, passando pela estrutura√ß√£o das pastas, at√© o upload dos arquivos CSV. A simplicidade e a efici√™ncia dessa automa√ß√£o n√£o s√≥ simplificaram o processo como tamb√©m ajudaram a entender melhor como integrar os diferentes componentes da AWS em um fluxo de trabalho coeso e bem estruturado.

Essa sprint, com sua combina√ß√£o de servi√ßos da AWS e containers, me mostrou como √© poss√≠vel trabalhar de maneira mais √°gil e escal√°vel, sem depender de configura√ß√µes locais, garantindo flexibilidade e controle sobre os dados na nuvem.

[Confira o 'readme' do desafio aqui!](Desafio/README.md)