# sprint 10 - provis√≥rio .....

## üíª Exerc√≠cios 

Nesta sprint 08, avan√ßamos para a terceira etapa de constru√ß√£o do **Desafio Final do Programa de Bolsas da Compass UOL** - a etapa 3 de 5.

O foco principal nesta sprint √© o uso do `Apache Spark`, explorando sua capacidade de manipula√ß√£o de dados em larga escala e integra√ß√£o com o *Data Lake*. Entre os destaques est√£o o processamento distribu√≠do e a an√°lise avan√ßada, utilizando tanto Python quanto SQL. Al√©m disso, abordamos pr√°ticas otimizadas para manipula√ß√£o de dados estruturados e semi-estruturados, como o uso dos formatos Parquet e CSV - al√©m dos JSONs da √∫ltima sprint -, que oferecem compacta√ß√£o eficiente e desempenho elevado para grandes volumes de dados.

As principais atividades dessa sprint, entre exerc√≠cios e desafio, incluem:
- Cria√ß√£o de datasets simulados utilizando Python e bibliotecas como random e names;
- Transforma√ß√£o e limpeza de dados com PySpark, explorando:
- Cria√ß√£o e manipula√ß√£o de DataFrames;
- Opera√ß√µes SQL sobre dados armazenados no formato Parquet;
- Enriquecimento do DataFrame com colunas como Pa√≠s, Ano de Nascimento e Gera√ß√£o.
- Integra√ß√£o com o Data Lake no AWS S3, consolidando os dados manipulados em uma estrutura confi√°vel e escal√°vel.

Essa etapa fortalece a base para an√°lises e visualiza√ß√µes futuras, garantindo um pipeline eficiente para o processamento de dados no contexto do desafio. Al√©m disso, ajustamos o reposit√≥rio Git para incluir as atualiza√ß√µes do exerc√≠cio de extra√ß√£o da API TMDB, realizado na sprint anterior.

Abaixo um pouco mais dos exerc√≠cios, de forma detalhada:
<br/>

## 1 - Exerc√≠cio: Gera√ß√£o de nomes aleat√≥rios (com *seed*) e massa de dados üßô‚Äç‚ôÇÔ∏è

#### Etapa 1: Aquecimento com N√∫meros Aleat√≥rios e Lista Reversa
O exerc√≠cio iniciou com a utiliza√ß√£o da biblioteca `random` do Python, onde geramos uma sequ√™ncia de n√∫meros aleat√≥rios dentro de um intervalo espec√≠fico. Em seguida, para explorar a manipula√ß√£o de listas, imprimimos a vers√£o reversa dessa sequ√™ncia. Esse "aquecimento" nos ajudou a familiarizar com opera√ß√µes simples, mas essenciais, para trabalhar com dados de forma program√°tica.

![lista reversa](../Sprint08/evidencias/2.geracao_massa_dados/1-etapa1.png)

<br/>

#### Etapa 2: Cria√ß√£o de um CSV com Nomes de Animais
Na segunda etapa, trabalhamos com uma lista de nomes de animais previamente criada. O objetivo foi organizar essa lista em ordem alfab√©tica e, a partir dela, gerar um arquivo `.csv`, no qual cada linha correspondia a um animal. Essa etapa refor√ßou o conceito de ordena√ß√£o e a pr√°tica de exportar dados em formato CSV, frequentemente usado em an√°lises e manipula√ß√µes de dados. Apesar de ainda ser parte do aquecimento, essa atividade introduziu a import√¢ncia de estruturar dados para exerc√≠cios mais complexos.

![csv e execu√ß√£o](../Sprint08/evidencias/2.geracao_massa_dados/2-etapa2(csv_e_execucao-terminal).png)

<br/>

#### Etapa 3: Gera√ß√£o de um Arquivo TXT com Milh√µes de Nomes
A etapa final elevou a complexidade do exerc√≠cio, exigindo a importa√ß√£o das bibliotecas `random` e `names`. Nosso objetivo foi gerar um arquivo `.txt` contendo milh√µes de registros com nomes completos. Aqui, o destaque foi o uso da fun√ß√£o **`seed`** da biblioteca `random`. Aprendemos que, ao definir uma semente, podemos garantir a reprodutibilidade dos resultados, o que √© fundamental em contextos como testes de software e replica√ß√£o de cen√°rios em diferentes ambientes. Esse aprendizado foi crucial, pois permitiu que, mesmo trabalhando com dados pseudoaleat√≥rios, fosse poss√≠vel obter sempre os mesmos resultados ao reutilizar a mesma semente. O arquivo gerado ser√° usado no pr√≥ximo exerc√≠cio, envolvendo `Spark`, dando continuidade ao processo de an√°lise em massa de dados.

Essas etapas serviram como uma introdu√ß√£o pr√°tica ao trabalho com dados em larga escala, estabelecendo fundamentos importantes para projetos mais avan√ßados.

![instala√ß√£o names](../Sprint08/evidencias/2.geracao_massa_dados/3-bib_names_installing.png)
![execu√ß√£o sucesso](../Sprint08/evidencias/2.geracao_massa_dados/4-nomes-aleatorios-gerados.png)
![final do txt, demonstrando a execu√ß√£o dos milhoes de nomes](../Sprint08/evidencias/2.geracao_massa_dados/5-print_txt_ultimas_linhas.png)

<br/>
<br/>

## 2 - Exerc√≠cio: Apache Spark ‚ú®‚ú®

Este c√≥digo, desenvolvido com `PySpark`, buscou processar dados de nomes aleat√≥rios gerados no exerc√≠cio anterior e enriquecer essas informa√ß√µes com a atribui√ß√£o de pa√≠ses, anos de nascimento, e gera√ß√µes correspondentes. O objetivo final foi realizar uma an√°lise agregada que demonstra a distribui√ß√£o de pessoas por pa√≠s e gera√ß√£o, utilizando t√©cnicas de manipula√ß√£o de dados, fun√ß√µes python e consultas SQL no ambiente Spark.

O exerc√≠cio foi realizado em 10 etapas com c√≥digos que foram se acumulando em funcionalidades, e eventualmente reproduzindo a mesma consulta em python ou SQL, tendo o mesmo SEED e portanto o mesmo resultado de consulta.

Aqui abaixo vamos explorar de forma fragmentada o √∫ltimo c√≥digo da 10a. etapa, que portanto, compreende todas as etapas anteriores - afim de otimizar esta documenta√ß√£o.

#### Configura√ß√£o Inicial e Prepara√ß√£o do Ambiente
O primeiro passo foi importar as bibliotecas necess√°rias, com destaque para `SparkSession` e expr do m√≥dulo `pyspark.sql.functions`. A configura√ß√£o da vari√°vel `SEED` garantiu a reprodutibilidade dos resultados, assegurando que as opera√ß√µes pseudoaleat√≥rias pudessem ser replicadas de forma id√™ntica. A `SparkSession` foi inicializada para executar opera√ß√µes distribu√≠das, configurando o ambiente de processamento local com todos os n√∫cleos dispon√≠veis - que no meu ambiente local s√£o 4 n√∫cleos reais e mais 4 virtuais.

```python
# Importando as bibliotecas necess√°rias
from pyspark.sql import SparkSession
from pyspark.sql.functions import expr

# Configurando a seed para garantir reprodutibilidade
SEED = 42

# Inicializando a SparkSession
spark = SparkSession.builder \
    .master("local[*]") \
    .appName("Laborat√≥rio Etapa 10 - Pessoas por Pa√≠s e Gera√ß√£o") \
    .getOrCreate()
```
<br/>

#### Carregamento e Estrutura√ß√£o dos Dados
Em seguida, o arquivo `names_aleatorios.txt` foi carregado em um *DataFrame*, com a coluna de nomes renomeada para "nome". A lista de pa√≠ses da Am√©rica do Sul foi fornecida para ser usada como base na atribui√ß√£o de nacionalidades fict√≠cias. A escolha desses pa√≠ses serviu para criar uma narrativa regional, tornando o conjunto de dados mais diversificado.

```python
# Carregando o arquivo names_aleatorios.txt no DataFrame
df_nomes = spark.read.text("names_aleatorios.txt").withColumnRenamed("value", "nome")

# Lista de pa√≠ses fornecida
paises = [
    "Argentina", "Bol√≠via", "Brasil", "Chile", "Col√¥mbia",
    "Equador", "Guiana", "Paraguai", "Peru", "Suriname",
    "Uruguai", "Venezuela", "Guiana Francesa"
]
```
<br/>

#### Enriquecimento dos Dados: Pa√≠ses e Anos de Nascimento
A coluna pa√≠s foi adicionada ao *DataFrame* de forma pseudoaleat√≥ria, utilizando a fun√ß√£o `rand()` em combina√ß√£o com o `SEED`. Isso garantiu uma distribui√ß√£o uniforme e consistente dos pa√≠ses ao longo dos registros. Em seguida, foi gerada a coluna ano_nascimento, com valores variando entre 1945 e 2010, simulando uma popula√ß√£o de diferentes faixas et√°rias. Esses valores foram calculados com base em um intervalo controlado, introduzindo diversidade temporal no conjunto de dados.

```python
# Adicionando a coluna 'pais' de forma aleat√≥ria baseada no seed
df_nomes = df_nomes.withColumn(
    "pais",
    expr(f"CASE MOD(CAST(rand({SEED}) * {len(paises)} AS INT), {len(paises)}) " +
         "WHEN 0 THEN 'Argentina' WHEN 1 THEN 'Bol√≠via' WHEN 2 THEN 'Brasil' " +
         "WHEN 3 THEN 'Chile' WHEN 4 THEN 'Col√¥mbia' WHEN 5 THEN 'Equador' " +
         "WHEN 6 THEN 'Guiana' WHEN 7 THEN 'Paraguai' WHEN 8 THEN 'Peru' " +
         "WHEN 9 THEN 'Suriname' WHEN 10 THEN 'Uruguai' WHEN 11 THEN 'Venezuela' " +
         "ELSE 'Guiana Francesa' END")
)

# Adicionando a coluna 'ano_nascimento' com valores pseudoaleat√≥rios entre 1945 e 2010
df_nomes = df_nomes.withColumn(
    "ano_nascimento",
    expr(f"CAST(1945 + FLOOR(rand({SEED}) * 66) AS INT)")
)
```
<br/>

##### Classifica√ß√£o por Gera√ß√£o
Com a coluna de anos de nascimento definida, foi criada a coluna geracao, categorizando os dados em quatro grupos et√°rios amplamente reconhecidos: Baby Boomers, Gera√ß√£o X, Millennials e Gera√ß√£o Z. Essa classifica√ß√£o foi baseada em crit√©rios hist√≥ricos, refletindo mudan√ßas demogr√°ficas e culturais entre os per√≠odos, numa simula√ß√£o da massa de dados.

```python
# Adicionando a coluna 'geracao' com base no ano de nascimento
df_nomes = df_nomes.withColumn(
    "geracao",
    expr("""
        CASE
            WHEN ano_nascimento BETWEEN 1945 AND 1964 THEN 'Baby Boomers'
            WHEN ano_nascimento BETWEEN 1965 AND 1979 THEN 'Geracao X'
            WHEN ano_nascimento BETWEEN 1980 AND 1994 THEN 'Millennials'
            ELSE 'Geracao Z'
        END
    """)
)
```
<br/>

#### An√°lise com Spark SQL
O *DataFrame* foi registrado como uma tabela tempor√°ria chamada nomes, permitindo que consultas `SQL` fossem realizadas diretamente sobre os dados. A consulta principal agrupou as informa√ß√µes por pa√≠s e gera√ß√£o, contabilizando o n√∫mero total de registros em cada combina√ß√£o. A ordena√ß√£o por pa√≠s, gera√ß√£o e total possibilitou uma apresenta√ß√£o clara e organizada dos resultados.

``` python
# Registrando o DataFrame como uma tabela tempor√°ria
df_nomes.createOrReplaceTempView("nomes")

# Consultando o n√∫mero de pessoas por pais e geracao usando Spark SQL
resultado = spark.sql("""
    SELECT pais, geracao, COUNT(*) AS total
    FROM nomes
    GROUP BY pais, geracao
    ORDER BY pais ASC, geracao ASC, total ASC
""")
```

<br/>

#### Apresenta√ß√£o dos Resultados
Ap√≥s realizar a consulta, o n√∫mero total de combina√ß√µes foi contado, e os resultados foram exibidos integralmente. Essa etapa serviu para validar a an√°lise, garantindo que todos os dados processados fossem apresentados de forma detalhada. O uso do m√©todo `show()` facilitou a visualiza√ß√£o das combina√ß√µes de pa√≠ses e gera√ß√µes, oferecendo *insights* claros sobre a distribui√ß√£o da popula√ß√£o simulada.

``` python
# Contando o n√∫mero total de linhas no resultado
num_linhas = resultado.count()

# Exibindo todos os resultados
print(f"Exibindo todas as combina√ß√µes de pa√≠ses e gera√ß√µes (total de {num_linhas} linhas):")
resultado.show(num_linhas, truncate=False)
```

<br/>
<br/>

## 3 - Exerc√≠cio: Extra√ß√£o de dados da API TMDB ü§ñ
Este exeerc√≠cio foi resolvido na sprint 07, e devidamente copiado no reposit√≥rio conforme instru√ß√£o.

[Aqui est√° o c√≥digo gerado para esta extra√ß√£o](../Sprint08/exercicios/5.TMDB/script.py), lembrando que foi utilizado um arquivo e biblioteca `.env`, como boa pr√°tica de n√£o exibir senhas e tokens no c√≥digo - e por conseguinte, inclu√≠do no `.gitignore` da pasta. 
<br/>
<br/>  

# üìú Certificados
Nesta sprint 08, n√£o houve nenhum curso obrigat√≥rio com disponibiliza√ß√£o de certificados.
<br/>  
<br/>  
  
# üß† Desafio
#### Camada Trusted: Transforma√ß√£o e Integra√ß√£o de Dados no Data Lake  

Nesta etapa do desafio, o foco principal √© a cria√ß√£o de uma **camada Trusted** no *Data Lake*, utilizando o `AWS Glue` para processar e transformar dados provenientes de diferentes fontes. O objetivo √© garantir que os dados estejam limpos, confi√°veis e prontos para an√°lises em ferramentas como o `AWS Athena`.

A abordagem envolve o uso de `Apache Spark` no Glue para desenvolver *jobs* que consolidam os dados da **camada Raw** em um formato padronizado e escal√°vel, persistido num *bucket* do `AWS S3`. Com essa estrutura, o *Data Lake* ser√° fortalecido para suportar consultas e visualiza√ß√µes otimizadas nas pr√≥ximas fases do projeto.

[Confira o 'readme' do desafio aqui!](../Sprint08/desafio/README.md)