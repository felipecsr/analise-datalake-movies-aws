## üéØ Objetivo üî¥üî¥üî¥
Este README, da sprint 08, √© o segundo de cinco etapas que comp√µem o **Desafio Final do Programa de Bolsas da Compass UOL**.

O desafio, em resumo, objetiva a constru√ß√£o de dashboard para an√°lises de dados provenientes de um Data Lake, com um pipeline completo envolvendo coleta, tratamento, an√°lise e visualiza√ß√£o de dados. A etapa atual focou na **extra√ß√£o de informa√ß√µes da API p√∫blica do TMDB** para complementar os dados locais, na camada de dados brutos, ainda n√£o filtrados e tratdos. Utilizamos tecnologias como AWS Lambda, AWS S3, AWS EventBridge e CloudWatch para garantir automa√ß√£o e efici√™ncia nessa coleta.

Nesta sprint, o objetivo principal foi implementar a extra√ß√£o de dados do TMDB, com foco nos g√™neros **Crime e Guerra**, seguindo crit√©rios definidos para garantir que os dados coletados estejam devidamente organizados e otimizados para armazenamento e an√°lises posteriores.

<br/>

## ‚úç Escopo das atividades - Etapa 2 do desafio
- Criar uma fun√ß√£o Lambda para extrair os dados do TMDB.
    > com limite de 15 minutos na execu√ß√£o da lambda 
- Foi necess√°rio a segmenta√ß√£o das consultas por g√™nero e recortes temporais de 5 em 5 anos para respeitar os limites da API.
    > limita√ß√£o de 500 p√°ginas por requisi√ß√£o e limita√ß√£o de 50 requisi√ß√µes por segundo
- Automatizar as execu√ß√µes utilizando par√¢metros (*eventos* na linguagem de AWS Lambda) de cada segmento agendamentos via `AWS EventBridge`.
    > com as entradas de g√™nero e per√≠odos de lan√ßamentos dos filmes
- Garantir que os dados extra√≠dos estejam organizados no bucket S3 em formato JSON, obedecendo √† estrutura de pastas definida.
    > organizados dentro da camada de dados brutos
- Testar localmente e na nuvem, ajustando o c√≥digo e os agendamentos conforme necess√°rio para otimizar o desempenho e respeitar os limites de execu√ß√£o da API e da fun√ß√£o Lambda.
    > arquivos com at√© 100 registros e no m√°ximo 10 MB

<br/>

## ‚ñ∂Ô∏è Resolu√ß√£o do desafio!

### ¬∑ Œª Desenvolvimento e ajustes no c√≥digo
A fun√ß√£o Lambda foi implementada com os seguintes pontos de destaque:

- Entrada de dados via eventos JSON, contendo o g√™nero ('id 80' para Crime e 'id 10752' para Guerra) e o ano inicial para o recorte de 5 anos.
- Pagina√ß√£o automatizada para lidar com o limite de 500 p√°ginas por consulta da API do TMDB.
- Armazenamento otimizado, com os dados extra√≠dos organizados em m√∫ltiplos arquivos JSON, cada um contendo no m√°ximo 100 registros para respeitar o limite de 10 MB por arquivo no S3.
- O c√≥digo foi testado localmente para garantir a funcionalidade e o tempo necess√°rio para a execu√ß√£o completa, considerando os limites de 15 minutos da fun√ß√£o Lambda.

#### Script de teste de extra√ß√£o (local)
``` python
import os
import json
import requests
import logging
import time
from datetime import datetime, timedelta
from dotenv import load_dotenv

# Carregar vari√°veis de ambiente do arquivo .env
load_dotenv()

# Configura√ß√£o de logs
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def fetch_movie_details(movie_id, tmdb_api_key):
    """Busca os detalhes de um filme espec√≠fico usando o ID."""
    url = f"https://api.themoviedb.org/3/movie/{movie_id}"
    params = {"api_key": tmdb_api_key}
    response = requests.get(url, params=params)
    response.raise_for_status()
    return response.json()

def fetch_movies_by_year(genre_id, year, tmdb_api_key, max_pages=500):
    """Busca filmes por g√™nero e ano-calend√°rio com todos os campos detalhados."""
    base_url = "https://api.themoviedb.org/3/discover/movie"
    all_movies = []

    start_date = f"{year}-01-01"
    end_date = f"{year}-12-31"

    for page in range(1, max_pages + 1):
        params = {
            "with_genres": genre_id,
            "primary_release_date.gte": start_date,
            "primary_release_date.lte": end_date,
            "page": page,
            "api_key": tmdb_api_key
        }
        try:
            logging.info(f"Buscando filmes - G√™nero: {genre_id}, Ano: {year}, P√°gina: {page}")
            response = requests.get(base_url, params=params, timeout=10)
            response.raise_for_status()
            data = response.json()
            movies = data.get("results", [])
            if not movies:
                logging.info("Nenhum filme encontrado ou p√°ginas esgotadas.")
                break

            for movie in movies:
                # Obter detalhes do filme
                movie_details = fetch_movie_details(movie["id"], tmdb_api_key)
                movie_data = {
                    "id_tmdb": movie_details.get("id"),
                    "title": movie_details.get("title"),
                    "release_date": movie_details.get("release_date"),
                    "overview": movie_details.get("overview"),
                    "poster_path": movie_details.get("poster_path"),
                    "backdrop_path": movie_details.get("backdrop_path"),
                    "production_companies": [
                        {"name": company["name"], "id": company["id"]} for company in movie_details.get("production_companies", [])
                    ],
                    "production_countries": [
                        {"iso_3166_1": country["iso_3166_1"], "name": country["name"]} for country in movie_details.get("production_countries", [])
                    ],
                    "spoken_languages": [
                        {"iso_639_1": language["iso_639_1"], "name": language["name"]} for language in movie_details.get("spoken_languages", [])
                    ],
                    "budget": movie_details.get("budget"),
                    "revenue": movie_details.get("revenue"),
                    "vote_average": movie_details.get("vote_average"),
                    "adult": movie_details.get("adult"),
                    "belongs_to_collection": {
                        "id": movie_details["belongs_to_collection"]["id"],
                        "name": movie_details["belongs_to_collection"]["name"]
                    } if movie_details.get("belongs_to_collection") else None,
                    "homepage": movie_details.get("homepage"),
                    "imdb_id": movie_details.get("imdb_id"),
                    "original_language": movie_details.get("original_language"),
                    "popularity": movie_details.get("popularity"),
                    "status": movie_details.get("status"),
                    "tagline": movie_details.get("tagline"),
                    "video": movie_details.get("video")
                }
                all_movies.append(movie_data)

            # Adicionando atraso para evitar bloqueios
            time.sleep(1)
        except requests.exceptions.RequestException as e:
            logging.error(f"Erro ao buscar filmes na p√°gina {page}: {e}")
            break

    return all_movies

def save_movies_locally(movies, genre_name, year):
    """Salva os filmes localmente em arquivos JSON por ano."""
    output_folder = "output"
    os.makedirs(output_folder, exist_ok=True)

    json_data = json.dumps(movies, ensure_ascii=False, indent=4)
    file_name = f"{output_folder}/{genre_name}_{year}.json"
    try:
        with open(file_name, "w", encoding="utf-8") as f:
            f.write(json_data)
        logging.info(f"Arquivo salvo localmente: {file_name}")
    except Exception as e:
        logging.error(f"Erro ao salvar o arquivo {file_name}: {e}")

def main():
    """Fun√ß√£o principal para execu√ß√£o local."""
    tmdb_api_key = os.getenv("TMDB_API_KEY")
    genres = {
        "Guerra": {"id": 10752, "years": range(1900, 2025)}  # Gera arquivos de 1900 a 2024
    }

    for genre_name, genre_data in genres.items():
        genre_id = genre_data["id"]
        for year in genre_data["years"]:
            logging.info(f"Processando filmes do g√™nero {genre_name} no ano {year}...")
            movies = fetch_movies_by_year(genre_id, year, tmdb_api_key)
            save_movies_locally(movies, genre_name, year)

if __name__ == "__main__":
    main()
```
Obs: *neste momento do teste j√° havia mapeado o dado `imdb_id`que servir√° como chave prim√°ria para integrar as base de dados, desta sprint e os dados orieundos dos IMDB da sprint 06.*

<br/>

#### Script de para mapeamento do volume de registros e p√°ginas (estudando a API e planajando o *script servless*)
```python
import os
import requests
from dotenv import load_dotenv

def get_total_pages_and_results(genre_id):
    # Carregar a chave da API do TMDB a partir do arquivo .env
    load_dotenv()
    TMDB_API_KEY = os.environ.get("TMDB_API_KEY")
    
    if not TMDB_API_KEY:
        raise ValueError("API Key do TMDB n√£o encontrada. Certifique-se de configur√°-la no arquivo .env.")

    # URL base do endpoint de descoberta
    base_url = "https://api.themoviedb.org/3/discover/movie"
    headers = {"Authorization": f"Bearer {TMDB_API_KEY}"}

    # Par√¢metros para a requisi√ß√£o inicial
    params = {
        "api_key": TMDB_API_KEY,
        "with_genres": genre_id,
        "page": 1  # Apenas a primeira p√°gina √© necess√°ria para obter as informa√ß√µes
    }

    # Fazer a requisi√ß√£o HTTP
    response = requests.get(base_url, headers=headers, params=params)
    if response.status_code != 200:
        raise ValueError(f"Erro na API TMDB: {response.status_code}, {response.text}")

    data = response.json()
    total_results = data.get("total_results", 0)
    total_pages = data.get("total_pages", 0)

    return total_results, total_pages

def main():
    genres = {
        "Crime": 80,
        "Guerra": 10752
    }

    for genre_name, genre_id in genres.items():
        try:
            total_results, total_pages = get_total_pages_and_results(genre_id)
            print(f"G√™nero: {genre_name}")
            print(f"  Total de filmes: {total_results}")
            print(f"  Total de p√°ginas: {total_pages}")
        except Exception as e:
            print(f"Erro ao obter informa√ß√µes para o g√™nero {genre_name}: {e}")

if __name__ == "__main__":
    main()
```
OBS: o resultado deste contador, no pr√≥prio terminal, em 27/12/2024 √†s 17h00, foi o seguinte:
``` bash
G√™nero: Crime
  Total de filmes: 35319
  Total de p√°ginas: 1766
G√™nero: Guerra
  Total de filmes: 11116
  Total de p√°ginas: 556
```

<br/>

### Script para execu√ß√£o como Fun√ß√£o Lambda
```python
import os
import json
import requests
import logging
import boto3
from datetime import datetime
from botocore.exceptions import NoCredentialsError

# Configura√ß√£o de logs com n√≠vel DEBUG para capturar todos os detalhes
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

# Cliente do S3
s3 = boto3.client('s3')

def fetch_movie_details(movie_id, tmdb_api_key):
    """Busca os detalhes de um filme espec√≠fico usando o ID."""
    logging.debug(f"Buscando detalhes do filme ID: {movie_id}")
    url = f"https://api.themoviedb.org/3/movie/{movie_id}"
    params = {"api_key": tmdb_api_key}
    try:
        response = requests.get(url, params=params)
        response.raise_for_status()
        logging.debug(f"Detalhes do filme {movie_id} recuperados com sucesso.")
        return response.json()
    except requests.exceptions.RequestException as e:
        logging.error(f"Erro ao buscar detalhes do filme {movie_id}: {e}")
        raise

def fetch_movies_by_interval(genre_id, start_year, end_year, tmdb_api_key, max_pages=500):
    """Busca filmes por g√™nero e intervalo de anos, capturando todas as p√°ginas."""
    base_url = "https://api.themoviedb.org/3/discover/movie"
    all_movies = []
    page = 1

    while True:
        params = {
            "with_genres": genre_id,
            "primary_release_date.gte": f"{start_year}-01-01",
            "primary_release_date.lte": f"{end_year}-12-31",
            "page": page,
            "api_key": tmdb_api_key
        }
        try:
            logging.info(f"Buscando filmes - G√™nero: {genre_id}, Intervalo: {start_year}-{end_year}, P√°gina: {page}")
            response = requests.get(base_url, params=params, timeout=10)
            response.raise_for_status()
            data = response.json()

            # Adiciona os filmes da p√°gina atual
            movies = data.get("results", [])
            all_movies.extend(movies)

            total_pages = data.get("total_pages", 0)
            logging.debug(f"P√°gina {page} de {total_pages} processada.")

            # Verifica se chegou na √∫ltima p√°gina
            if page >= total_pages or page >= max_pages:
                logging.info(f"Busca completa: total de {len(all_movies)} filmes capturados.")
                break

            # Avan√ßa para a pr√≥xima p√°gina
            page += 1

        except requests.exceptions.RequestException as e:
            logging.error(f"Erro ao buscar filmes na p√°gina {page}: {e}")
            break

    return all_movies

def save_to_s3(bucket_name, folder_path, filename, data):
    """Salva o JSON no S3."""
    file_path = f"{folder_path}/{filename}"
    try:
        logging.info(f"Tentando salvar arquivo {filename} no bucket {bucket_name}.")
        s3.put_object(Bucket=bucket_name, Key=file_path, Body=json.dumps(data, ensure_ascii=False, indent=4))
        logging.info(f"Arquivo salvo no S3: s3://{bucket_name}/{file_path}")
    except NoCredentialsError:
        logging.error("Credenciais AWS n√£o configuradas corretamente.")
        raise
    except Exception as e:
        logging.error(f"Erro ao salvar arquivo no S3: {e}")
        raise

def lambda_handler(event, context):
    """Handler principal do AWS Lambda."""
    logging.debug("Iniciando execu√ß√£o do lambda_handler.")
    logging.debug(f"Evento recebido: {event}")

    tmdb_api_key = os.getenv("TMDB_API_KEY")
    if not tmdb_api_key:
        logging.error("Chave da API do TMDb n√£o configurada.")
        raise ValueError("Chave da API do TMDb n√£o configurada nas vari√°veis de ambiente.")
    
    bucket_name = "desafio-filmes-series"
    base_path = f"Raw/TMDB/JSON/Movies/{datetime.now().strftime('%Y/%m/%d')}"

    # Valida√ß√£o de par√¢metros no evento
    if "start_year" not in event or "genre_id" not in event:
        logging.error("Os par√¢metros 'start_year' e 'genre_id' s√£o obrigat√≥rios.")
        raise KeyError("Os par√¢metros 'start_year' e 'genre_id' s√£o obrigat√≥rios no evento.")

    start_year = int(event["start_year"])
    genre_id = event["genre_id"]
    end_year = start_year + 4  # Ajustando para intervalos de 5 anos
    logging.debug(f"Par√¢metros processados: start_year={start_year}, end_year={end_year}, genre_id={genre_id}")

    logging.info(f"Processando filmes do g√™nero {genre_id} para o intervalo {start_year}-{end_year}.")

    try:
        movies = fetch_movies_by_interval(genre_id, start_year, end_year, tmdb_api_key)
        logging.info(f"N√∫mero total de filmes processados: {len(movies)}.")

        detailed_movies = []
        for movie in movies:
            try:
                movie_id = movie.get("id")
                movie_details = fetch_movie_details(movie_id, tmdb_api_key)
                detailed_movies.append({
                    "id_tmdb": movie_details.get("id"),
                    "title": movie_details.get("title"),
                    "release_date": movie_details.get("release_date"),
                    "overview": movie_details.get("overview"),
                    "poster_path": movie_details.get("poster_path"),
                    "backdrop_path": movie_details.get("backdrop_path"),
                    "production_companies": [
                        {"name": company["name"], "id": company["id"]} for company in movie_details.get("production_companies", [])
                    ],
                    "production_countries": [
                        {"iso_3166_1": country["iso_3166_1"], "name": country["name"]} for country in movie_details.get("production_countries", [])
                    ],
                    "spoken_languages": [
                        {"iso_639_1": language["iso_639_1"], "name": language["name"]} for language in movie_details.get("spoken_languages", [])
                    ],
                    "budget": movie_details.get("budget"),
                    "revenue": movie_details.get("revenue"),
                    "vote_average": movie_details.get("vote_average"),
                    "adult": movie_details.get("adult"),
                    "belongs_to_collection": {
                        "id": movie_details["belongs_to_collection"]["id"],
                        "name": movie_details["belongs_to_collection"]["name"]
                    } if movie_details.get("belongs_to_collection") else None,
                    "homepage": movie_details.get("homepage"),
                    "imdb_id": movie_details.get("imdb_id"),
                    "original_language": movie_details.get("original_language"),
                    "popularity": movie_details.get("popularity"),
                    "status": movie_details.get("status"),
                    "tagline": movie_details.get("tagline"),
                    "video": movie_details.get("video")
                })
            except Exception as e:
                logging.error(f"Erro ao buscar detalhes para o filme ID {movie.get('id')}: {e}")

        batch_number = 1

        # Divis√£o e salvamento dos registros detalhados
        for i in range(0, len(detailed_movies), 100):
            chunk = detailed_movies[i:i + 100]
            filename = f"genre-{genre_id}-{start_year}-{end_year}-{batch_number}.json"
            logging.info(f"Criando lote {batch_number}: {len(chunk)} registros ser√£o salvos no arquivo {filename}.")
            save_to_s3(bucket_name, base_path, filename, chunk)
            logging.info(f"Lote {batch_number} salvo com sucesso: {len(chunk)} registros no arquivo {filename}.")
            batch_number += 1

        logging.info(f"Processamento completo para o intervalo {start_year}-{end_year}. Total de arquivos gerados: {batch_number - 1}.")
    except Exception as e:
        logging.error(f"Erro durante o processamento: {e}")
        raise
    finally:
        logging.debug("lambda_handler conclu√≠do com sucesso.")
```

<br/>

## üåê Testes e agendamento no AWS

### Testes locais

Inicialmente, foram realizados testes para verificar o tempo de execu√ß√£o e a estrutura dos arquivos gerados.
Identificou-se que uma √∫nica consulta poderia exceder o limite de 15 minutos quando da execu√ß√£o via `AWS Lambda`. Assim, foram adotados recortes de 5 anos por execu√ß√£o.

### Configura√ß√£o do agendamento no AWS:

Foi criado mais um script em python, para execu√ß√£o via `AWS CLI` para agendar 50 execu√ß√µes autom√°ticas, cada uma com intervalo de 1 minuto, para evitar atingir os limites de requisi√ß√µes e p√°ginas da API.  

Cada execu√ß√£o enviava um evento JSON para a fun√ß√£o Lambda, com informa√ß√µes do g√™nero e do per√≠odo de 5 anos a ser extra√≠do.

```python
import os
import json
import logging
import boto3
from datetime import datetime, timedelta, timezone

# Configura√ß√£o de logs
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

# Cliente AWS Scheduler
scheduler = boto3.client('scheduler')

# Configura√ß√µes iniciais
lambda_arn = "arn:aws:lambda:us-east-1:767828750921:function:extract_tmdb_crimes_war"  # Substitua pelo ARN correto
role_arn = "arn:aws:iam::767828750921:role/service-role/extract_tmdb_crimes_war-role-6jvjnxv6"  # Substitu√≠do pelo ARN correto com ID da conta e nome do role
region = "us-east-1"
genres = [
    {"id": 80, "name": "Crime"},       # G√™nero Crime
    {"id": 10752, "name": "Guerra"}   # G√™nero Guerra
]
decades = list(range(1900, 2025, 5))  # Intervalos de 5 anos de 1900 at√© 2024

# Configura√ß√£o de tempo
start_time = datetime(2024, 12, 27, 16, 55, tzinfo=timezone.utc)  # Ajustado para UTC correspondente ao hor√°rio local (UTC-3, 15h02 no Brasil)
interval_minutes = 1  # Intervalo de 1 minuto entre os cronogramas

# Loop para criar cronogramas
for genre_index, genre in enumerate(genres):
    for decade_index, start_year in enumerate(decades):
        # Nome do cronograma
        schedule_name = f"Schedule_{genre['name']}_{start_year}-{start_year + 4}"

        # Payload para a Lambda
        payload = {
            "start_year": start_year,
            "genre_id": genre["id"]
        }

        # Calcular o hor√°rio para cada execu√ß√£o
        schedule_time = start_time + timedelta(minutes=(genre_index * len(decades) + decade_index) * interval_minutes)
        cron_expression = f"cron({schedule_time.minute} {schedule_time.hour} {schedule_time.day} {schedule_time.month} ? {schedule_time.year})"

        # Criar o cronograma
        try:
            logging.info(f"Criando cronograma: {schedule_name} com cronograma {cron_expression}...")
            scheduler.create_schedule(
                Name=schedule_name,
                ScheduleExpression=cron_expression,
                FlexibleTimeWindow={"Mode": "OFF"},
                Target={
                    "Arn": lambda_arn,
                    "RoleArn": role_arn,
                    "Input": json.dumps(payload)
                }
            )
            logging.info(f"Cronograma criado com sucesso: {schedule_name}")
        except Exception as e:
            logging.error(f"Erro ao criar cronograma {schedule_name}: {e}")
```

![evid√™ncia dos agendamentos](../evidencias/desafio/1-scheduler.png)

### Armazenamento no S3:

Os dados foram organizados em um diret√≥rio espec√≠fico dentro do bucket desafio-filmes-series, com a seguinte estrutura, e organizados em **473 objetos** (arquivos `.json`):

> s3://desafio-filmes-series/Raw/TMDB/JSON/Movies/2024/12/27/

<br/>

![arquivos armazenados no bucket](../evidencias/desafio/2-json_extracted.png)

## üîç Monitoramento
O processo foi monitorado via CloudWatch para garantir que todas as execu√ß√µes ocorreram com sucesso. Tamb√©m foi validado se os arquivos gerados estavam sendo salvos corretamente no bucket S3.

Neste ponto houveram dificuldades que n√£o consegui superar, sobre logs detalhados de execu√ß√£o. Infelizmente s√≥ consegui inicio e fim da execu√ß√£o, sem detalhamentos de monitoramento.

<br/>

## üíæ Resultados
### Tempo de execu√ß√£o otimizado: 
Cada execu√ß√£o da fun√ß√£o Lambda foi configurada para processar apenas 5 anos, garantindo que o tempo m√°ximo de 15 minutos n√£o fosse excedido.

### Dados organizados: 
Arquivos JSON de at√© 100 registros, com tamanho m√©dio de 150 KB, armazenados em uma estrutura l√≥gica no S3.

### Automa√ß√£o bem-sucedida: 
A integra√ß√£o com EventBridge garantiu que os agendamentos ocorreram sem falhas.

<br/>

## üìå Considera√ß√µes finais sobre a Sprint 07

A Sprint 07 foi desafiadora e enriquecedora, com foco em compreender e utilizar os servi√ßos AWS para extrair e organizar dados de forma eficiente. Essa etapa consolidou o aprendizado sobre:

- Configura√ß√£o e uso de fun√ß√µes Lambda para tarefas autom√°ticas e escal√°veis.
- Gerenciamento de limites de APIs p√∫blicas e ajustes necess√°rios para otimizar processos.
- Organiza√ß√£o de dados no S3, criando uma base s√≥lida para as pr√≥ximas etapas do desafio.

O resultado alcan√ßado √© um pipeline funcional e confi√°vel para a extra√ß√£o de dados do TMDB, atendendo aos requisitos do desafio e estabelecendo uma base s√≥lida para an√°lises futuras.

Estou motivado para avan√ßar para as pr√≥ximas sprints, onde os dados coletados ser√£o tratados e utilizados para a cria√ß√£o de dashboards anal√≠ticos no AWS QuickSight. üöÄ
